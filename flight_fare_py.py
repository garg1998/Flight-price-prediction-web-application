# -*- coding: utf-8 -*-
"""Flight_fare.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UlpteYs_-uYKY1YvfQ8bIUS0Yj_ILJli
"""

# importing all necessary libraries

import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import seaborn as sns #for visualization

import matplotlib.pyplot as plt #for visualization

# Importing the training dataset
flight_df=pd.read_excel('Data_Train.xlsx')
flight_df.head()

flight_df.info()

"""
So, we have 10 object datatype columns and only one int. In order to use these columns for predicition purposes, we will need to encode them. But first, let's see if we have any null values"""

flight_df['Price'].describe()

flight_df.isnull().sum()

"""Since, we have only one null value in destination and total_stops, we'll just discard them

"""

flight_df.dropna(inplace=True)

flight_df.isnull().sum()

flight_df[flight_df.duplicated()]

"""there are 220 duplicate rows. Before proceeding forward, we'll remove them. Also, Delhi and New Delhi are same, so I'll be renaming Delhi to New Delhi and check for duplicates again.

"""

flight_df['Source'] = flight_df['Source'].replace({'Delhi': 'New Delhi'})
flight_df['Destination'] = flight_df['Destination'].replace({'Delhi': 'New Delhi'})

# Removing all duplicates
flight_df.drop_duplicates(keep=False, inplace=True)

# Lets visualize boxplots to see how price is getting varied with the factors
#airline vs price
sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="boxen", height = 6, aspect = 3)
plt.show()

#airline vs total stops
sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="bar", hue='Total_Stops',height = 6, aspect = 3)
plt.show()

sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="bar", hue='Destination',height = 6, aspect = 3)
plt.show()

sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="bar", hue='Source',height = 6, aspect = 3)
plt.show()

flight_df['source_destination']=flight_df['Source']+'-'+flight_df['Destination']
flight_df.head()

sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="boxen", hue='source_destination',height = 6, aspect = 3)
plt.show()

# convert date related columns to datetime format like date_journey, Dep_Time etc
# Date_of_Journey
flight_df["Journey_day"] = pd.to_datetime(flight_df.Date_of_Journey, format="%d/%m/%Y").dt.day
flight_df["Journey_month"] = pd.to_datetime(flight_df["Date_of_Journey"], format = "%d/%m/%Y").dt.month
flight_df.drop(["Date_of_Journey"], axis = 1, inplace = True)

# Dep_Time
flight_df["Dep_hour"] = pd.to_datetime(flight_df["Dep_Time"]).dt.hour
flight_df["Dep_min"] = pd.to_datetime(flight_df["Dep_Time"]).dt.minute
flight_df.drop(["Dep_Time"], axis = 1, inplace = True)

# Arrival_Time
flight_df["Arrival_hour"] = pd.to_datetime(flight_df.Arrival_Time).dt.hour
flight_df["Arrival_min"] = pd.to_datetime(flight_df.Arrival_Time).dt.minute
flight_df.drop(["Arrival_Time"], axis = 1, inplace = True)

# Duration

duration=list(flight_df['Duration']) # convertion duration coulmn to list to sepearte hours and min

for i in range(len(duration)):
    if duration[i].split()!=2:
        if 'h' in duration[i]:
            duration[i]= duration[i].strip()+' 0m'

        else:
            duration[i]= '0h '+ duration[i]

duration_hr=[]
duration_min=[]

for i in range(len(duration)):
    duration_hr.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_min.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration

flight_df.drop(["Duration"], axis = 1, inplace = True)

flight_df.head()

sns.pairplot(flight_df)

"""Since we have the duration and total stops, we can drop the route column"""

# droppping route and source_destination
flight_df.drop(['Route'],axis=1,inplace=True)
flight_df.drop(['source_destination'],axis=1,inplace=True)

sns.catplot(y = "Price", x = "Airline", data = flight_df.sort_values("Price", ascending = False),kind="bar", hue='Journey_month',height = 6, aspect = 3)
plt.show()

sns.catplot(y = "Price", x = "Journey_month", data = flight_df.sort_values("Price", ascending = False),kind="bar", hue='Journey_day',height = 6, aspect = 3)
plt.show()

"""From the above analysis, following observations could be drawn:


*   Maximum price is for Jet airways business flights
*   Majority of the flights with one stops have are more expensive

*   All flights prices seem to be maximum in the month of march
*   Also, prices are higher for flights flying on the 1st day of the month







"""

# There are 6 categorical variables described below:
flight_df['Airline'].unique()

flight_df['Source'].unique()

flight_df['Destination'].unique()

flight_df['Additional_Info'].unique()

flight_df['Additional_Info'].value_counts()/flight_df['Additional_Info'].count()

# Almost 80% of the values contain no Info. Hence, we'll be dropping it
flight_df.drop(['Additional_Info'],axis=1,inplace=True)

flight_df['Total_Stops'].unique()

# one-hot encoding of categorical variables
print("Airline")
print("-"*75)
print(flight_df["Airline"].value_counts())
Airline = pd.get_dummies(flight_df["Airline"], drop_first= True)

print(flight_df["Source"].value_counts())
Source = pd.get_dummies(flight_df['Source'], drop_first= True)
Source = Source.add_suffix('_source')

print(flight_df["Destination"].value_counts())
Destination = pd.get_dummies(flight_df['Destination'], drop_first = True)
Destination = Destination.add_suffix('_destination')

fight_df = pd.concat([flight_df, Airline, Source, Destination], axis = 1)

fight_df.columns

# Replacing Total_Stops
fight_df.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)



flight_df.head()

fight_df.drop(['Airline','Source','Destination'],axis=1,inplace=True)
flight_df.head()

"""Now that we have created our final dataset, we'll need to divide it into dependant and independant variables and do feture selection"""

# X: indpendant variables and Y: dependant variables

X=fight_df.drop(['Price'],axis=1,inplace=False)

Y=fight_df['Price']
X.columns

# There are 3 feature selections methods we'll be exploring:
# 1. univariate analysis
# 2. correlation matrix
# 3. feature importance

# univariate analysis
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

fs=SelectKBest(score_func=chi2)
# Applying feature selection
X_selected=fs.fit(X,Y)

# graph of features

plt.figure(figsize=(15,15))
feat_importances = pd.Series(X_selected.scores_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

# correaltion matrix
plt.figure(figsize=(25,25))
sns.heatmap(fight_df.corr(),annot = True, cmap = "RdYlGn")

# Removing correlated features
Threshold=0.8
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

correlation(X,Threshold)

# Feature Selection

# Important feature using ExtraTreesRegressor

from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X, Y)

#plot graph of feature importances for better visualization
plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

sns.pairplot(flight_df)

# Making the same changes in test data

test=pd.read_excel('Test_set.xlsx')
test.isnull().sum()


test['Source'] = test['Source'].replace({'Delhi': 'New Delhi'})
test['Destination'] = test['Destination'].replace({'Delhi': 'New Delhi'})

# Date_of_Journey
test["Journey_day"] = pd.to_datetime(test.Date_of_Journey, format="%d/%m/%Y").dt.day
test["Journey_month"] = pd.to_datetime(test["Date_of_Journey"], format = "%d/%m/%Y").dt.month
test.drop(["Date_of_Journey"], axis = 1, inplace = True)

# Dep_Time
test["Dep_hour"] = pd.to_datetime(test["Dep_Time"]).dt.hour
test["Dep_min"] = pd.to_datetime(test["Dep_Time"]).dt.minute
test.drop(["Dep_Time"], axis = 1, inplace = True)

# Arrival_Time
test["Arrival_hour"] = pd.to_datetime(test.Arrival_Time).dt.hour
test["Arrival_min"] = pd.to_datetime(test.Arrival_Time).dt.minute
test.drop(["Arrival_Time"], axis = 1, inplace = True)

# Duration
duration=list(test['Duration']) # convertion duration coulmn to list to sepearte hours and min

for i in range(len(duration)):
    if duration[i].split()!=2:
        if 'h' in duration[i]:
            duration[i]= duration[i].strip()+' 0m'

        else:
            duration[i]= '0h '+ duration[i]

duration_hr=[]
duration_min=[]

for i in range(len(duration)):
    duration_hr.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_min.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration

test.drop(["Duration"], axis = 1, inplace = True)

# one-hot encoding of categorical variables
print("Airline")
print("-"*75)
print(test["Airline"].value_counts())
Airline = pd.get_dummies(test["Airline"], drop_first= True)

print(test["Source"].value_counts())
Source = pd.get_dummies(test["Source"], drop_first= True)


print(test["Destination"].value_counts())
Destination = pd.get_dummies(test["Destination"], drop_first = True)

Source = Source.add_suffix('_source')
Destination = Destination.add_suffix('_destination')

# Replacing Total_Stops
test.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)

test.drop(['Airline','Source','Destination','Route'],axis=1,inplace=True)

test = pd.concat([test, Airline, Source, Destination], axis = 1)

test.drop(['Additional_Info'],axis=1,inplace=True)

"""Regression modelling
I am going to apply 10 regression models and judge them on the basis of RMSE value since the value is not much influenced by outliers.
The models are:
* LinearRegression
* LGBM Regressor
* XGBoost Regressor
* CatBoost Regressor
* Stochastic Gradient Descent Regression
* Kernel Ridge Regression
* Support Vector Machine
* Random Forest Regressor
* Gradient Boosting Regression
* Bayesian Ridge Regression
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)

# importing ML models
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR

from sklearn.ensemble import GradientBoostingRegressor

from sklearn.linear_model import BayesianRidge

from sklearn.linear_model import LinearRegression

from sklearn.linear_model import ElasticNet

from xgboost.sklearn import XGBRegressor

from sklearn.kernel_ridge import KernelRidge
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

models = [['LinearRegression : ', LinearRegression()],
          ['ElasticNet :', ElasticNet()],
          ['DecisionTreeRegressor : ', DecisionTreeRegressor()],
          ['RandomForestRegressor : ', RandomForestRegressor()],
          ['SVR : ', SVR()],
          ['GradientBoostingRegressor : ', GradientBoostingRegressor()],
          ['KNeighborsRegressor : ', KNeighborsRegressor()],
          ['BayesianRidge : ', BayesianRidge()],
          ['KernalRidge: ', KernelRidge()],
           ['XGBoostRegressor: ', XGBRegressor()]]

rmse_score=[]
for name,model in models:
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    rmse_score.append([name,np.sqrt(mean_squared_error(y_test, predictions))])
    print(name, (np.sqrt(mean_squared_error(y_test, predictions))))

best_model=DecisionTreeRegressor()
best_model.fit(X_train, y_train)
prediction = best_model.predict(X_train)

best_model.score(X_train, y_train),best_model.score(X_test, y_test)

rmse_score=sorted(rmse_score,key=lambda x: x[1])
rmse_score
df=pd.DataFrame(rmse_score,columns=['model','rmse_score'])
df.head(3)

"""
Above 3 models have the lowest RMSE. Hence, we will be considering them for hyperparameter tuning

HyperParameter Tuning

* GridSearchCV -> computationally expensive
* RandomGridSearchCV-> Faster
"""

model_para={
    'RandomForestRegressor':{
        'model':RandomForestRegressor(),
        'parameters':{
            'n_estimators' : [300, 500, 700, 1000, 2100],
            'max_depth' : [3, 5, 7, 9, 11, 13, 15],
            'max_features' : ["auto", "sqrt", "log2"],
            'min_samples_split' : [2, 4, 6, 8]
        }
    },

    'GradientBoostingRegressor':{
        'model':GradientBoostingRegressor(),
        'parameters':{
            'learning_rate' : [0.05, 0.08, 0.1, 0.20, 0.25, 0.30],
            'n_estimators' : [300, 500, 700, 1000, 2100],
            'criterion' : ['friedman_mse', 'mse']
        }


    },

    'XGBoostRegressor':{
        'model':XGBRegressor(),
        'parameters':{
            'n_estimators' : [300, 500, 700, 1000, 2100],
            'max_depth' : [3, 5, 7, 10],
            'learning_rate' : [0.05, 0.08, 0.1, 0.20, 0.25, 0.30]
        }
    }


}

from sklearn.model_selection import RandomizedSearchCV

score = []

for name, mp in model_para.items() :
    rs = RandomizedSearchCV(estimator = mp['model'], param_distributions = mp['parameters'], cv = 5, n_jobs=-1,verbose=2)
    rs.fit(X_train, y_train)
    score.append({
        'model': name,
        'score' : rs.best_score_,
        'params' : rs.best_params_
    })

df=pd.DataFrame(score)
df

df['params'][2]

best_model=XGBRegressor(n_estimators= 700, learning_rate= 0.05, max_depth= 5)
best_model.fit(X_train, y_train)
prediction = best_model.predict(X_test)

best_model.score(X_train, y_train),best_model.score(X_test, y_test)

np.sqrt(mean_squared_error(y_test, prediction))

plt.figure(figsize = (8,8))
plt.scatter(y_test, prediction, alpha = 0.5)
plt.xlabel("Actual Values")
plt.ylabel("Prdicted Values")
plt.show()

